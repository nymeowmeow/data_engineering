{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Immigration Data Modelling\n",
    "\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "The aim of this project is to build a data processing pipeline, that import i94 immigration related data from various data source, transform and clean the data before storing to a data warehouse designed for immigration data.\n",
    "\n",
    "The data warehouse will support various queries to allow user to gain insight to immigration related problems, such as travel purpose, vsia type and other more complex queries.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import configparser\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from sqlalchemy import create_engine\n",
    "from pyspark.sql.functions import udf, col, monotonically_increasing_id, split, trim, asc, first, upper, desc, initcap, to_date, mean, lit\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, dayofweek\n",
    "from pyspark.sql.types import TimestampType, StringType, DoubleType, DecimalType, IntegerType\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(\"dl.cfg\")\n",
    "\n",
    "AWS_ACCESS_KEY_ID = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "AWS_SECRET_ACCESS_KEY = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "HOST = config['CLUSTER']['HOST']\n",
    "DB_NAME = config['CLUSTER']['DB_NAME']\n",
    "DB_USER = config['CLUSTER']['DB_USER']\n",
    "DB_PASSWORD = config['CLUSTER']['DB_PASSWORD']\n",
    "DB_PORT = config['CLUSTER']['DB_PORT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create spark session\n",
    "spark = SparkSession.builder\\\n",
    "                    .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "                    .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The aim of this project is to create a data warehouse for the immigration related data. The data will be loaded to the fact and dimension tables. The following dataset will be used.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "1. I94 immigration data, https://travel.trade.gov/research/reports/i94/historical/2016.html . The data is provied as sas7bat format with the following schema\n",
    "\n",
    "| Field name | Description |\n",
    "| ---------- | ----------- |\n",
    "| CICID | Primary Key |\n",
    "| I94YR | Year |\n",
    "| I94MON | Month |\n",
    "| I94CIT | Country of CitizenShip |\n",
    "| I94RES | Country of Residency |\n",
    "| I94PORT | Port of Arrival |\n",
    "| ARRDATE | Arrivate Date |\n",
    "| I94MODE | Transportation Mode |\n",
    "| I94ADDR | State of Arrival |\n",
    "| DEPDATE | Departure Date |\n",
    "| I94BIR | Age |\n",
    "| I94VISA | Visa Category |\n",
    "| COUNT | Number of people |\n",
    "| DTADFILE | Character Date |\n",
    "| VISAPOST | Department issuing Visa |\n",
    "| OCCUP | Occupation |\n",
    "| ENTDEPA | Arrival Flag |\n",
    "| ENTDEPD | Departure Flag |\n",
    "| ENTDEPU | Update Flag |\n",
    "| MATFLAG | Match Flag |\n",
    "| BIRYEAR | Year of Birth |\n",
    "| DTADDTO | Character Date |\n",
    "| GENDER | Gender |\n",
    "| INSNUM | INS Number |\n",
    "| AIRLINE | Airline |\n",
    "| ADMNUM | Admission Number |\n",
    "| FLTNO | Flight Number |\n",
    "| VISATYPE | Visa Type |\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sas7bdat files: 12\n"
     ]
    }
   ],
   "source": [
    "#go thru all the sas7bbat file\n",
    "i94_files = []\n",
    "for root, directory, files in os.walk('../../data'):\n",
    "    for file in files:\n",
    "        if file.endswith('.sas7bdat'):\n",
    "            i94_files.append(os.path.join(root, file))\n",
    "print (f'number of sas7bdat files: {len(i94_files)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n",
      "number of records: 3096313\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|  1.0|    null|    null| null|      T|   null|      U|   null| 1979.0|10282016|  null|  null|   null| 1.897628485E9| null|      B2|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null|  3.73679633E9|00296|      F1|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS|  6.66643185E8|   93|      B2|\n",
      "| 16.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|  28.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1988.0|09302016|  null|  null|     AA|9.246846133E10|00199|      B2|\n",
      "| 17.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|   4.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 2012.0|09302016|  null|  null|     AA|9.246846313E10|00199|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94Df = spark.read.format('com.github.saurfang.sas.spark').load(i94_files[0])\n",
    "i94Df.printSchema()\n",
    "print(f\"number of records: {i94Df.count()}\")\n",
    "i94Df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "2. World temperature data, https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data . The data is provided as csv file with the following schema\n",
    "\n",
    "|                      Field Name |                             Description | Sample Data |\n",
    "| ------------------------------- | --------------------------------------- | ----------- |\n",
    "| dt                              |                                    Date |  1743-11-01 |\n",
    "| Average Temperature             |             Monthly Average Temperature |       6.068 |\n",
    "| Average Temperature Uncertainty | Monthly Average Temperature Uncertainty |       1.737 |\n",
    "| City                            |                                    City |       Århus |\n",
    "| Country                         |                                 Country |     Denmark |\n",
    "| Latitude                        |                                Latitude |      57.05N |\n",
    "| Longitude                       |                               Longitude |      10.33E | \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: timestamp (nullable = true)\n",
      " |-- AverageTemperature: double (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n",
      "number of records: 8599212\n",
      "+-------------------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|                 dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+-------------------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01 00:00:00|             6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01 00:00:00|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01 00:00:00|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-02-01 00:00:00|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-03-01 00:00:00|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "+-------------------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weatherDf = spark.read.format('csv').option('sep', \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load('GlobalLandTemperaturesByCity.csv')\n",
    "weatherDf.printSchema()\n",
    "print (f'number of records: {weatherDf.count()}')\n",
    "weatherDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "3. U.S. city demographics data, https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/ . The data is provided as csv file with the following schema\n",
    "\n",
    "|             Field Name |                     Description | Sample Data | \n",
    "| ---------------------- | ------------------------------- | ----------- |\n",
    "|                   City |                            City |      Newark |\n",
    "|                  State |                           State |  New Jersey |\n",
    "|             Median Age |    Median Age of the population |        34.6 |\n",
    "|        Male Population |         Population that is Male |      138040 |\n",
    "|      Female Population |       Population that is Female |      143873 |\n",
    "|       Total Popultaion |          Population of the city |      281913 |\n",
    "|     Number of Veterans |          Population of Veterans |        5829 |\n",
    "|           Foreign-born | Population that is foreign-born |       86253 |\n",
    "| Average HouseHold Size |       Average size of household |        2.73 |\n",
    "|             State Code |                   US State Code |          NJ |\n",
    "|                   Race |                            Race |       White |\n",
    "|                  Count |          Population of the Race |       76402 |\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: double (nullable = true)\n",
      " |-- Male Population: integer (nullable = true)\n",
      " |-- Female Population: integer (nullable = true)\n",
      " |-- Total Population: integer (nullable = true)\n",
      " |-- Number of Veterans: integer (nullable = true)\n",
      " |-- Foreign-born: integer (nullable = true)\n",
      " |-- Average Household Size: double (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n",
      "number of records: 2891\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|        State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race|Count|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|          Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White|58723|\n",
      "|          Hoover|      Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|Rancho Cucamonga|   California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "|          Newark|   New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White|76402|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographicsDf = spark.read.format('csv').option('sep', \";\").option('inferSchema', \"true\").option(\"header\", \"true\").load(\"us-cities-demographics.csv\")\n",
    "demographicsDf.printSchema()\n",
    "print (f'number of records: {demographicsDf.count()}')\n",
    "demographicsDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "4. Airport Code Table, https://datahub.io/core/airport-codes#data . The data is provided as csv file with the following schema\n",
    "\n",
    "|   Field Name |                           Description |                        Sample Data |\n",
    "| ------------ | ------------------------------------- | ---------------------------------- |\n",
    "|        ident |                    airport identifier |                                00A |\n",
    "|         type |                       type of airport |                           heliport |\n",
    "|         name |                       name of airport |                  Total Rf Heliport |\n",
    "| elevation_ft |                             Elevation |                                 11 |\n",
    "|    continent |                             Continent |                                 NA |\n",
    "|  iso_country |                      iso country code |                                 US |\n",
    "|   iso_region |      country and state of the airport |                              US-PA |\n",
    "| municipality |                       city of airport |                           Bensalem |\n",
    "|     gps_code |                              GPS Code |                                00A |\n",
    "|    iata_code |                        IATA code      |                                    |\n",
    "|   local_code |                       local code      |                                00A |\n",
    "|  coordinates | latitude and longitude of the airport | -74.93360137939453, 40.07080078125 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "number of records: 55075\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|     US-AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|         US|     US-AR|     Newport|    null|     null|      null| -91.254898, 35.6087|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airportDf = spark.read.format('csv').option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(\"airport-codes_csv.csv\")\n",
    "airportDf.printSchema()\n",
    "print (f\"number of records: {airportDf.count()}\")\n",
    "airportDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#extract detail information from imigration label file\n",
    "\n",
    "country = { 'code' : [], 'country' : []}\n",
    "port = { 'code': [], 'city' : [], 'state' : []}\n",
    "mode = { 'code' : [], 'mode' : []}\n",
    "address = { 'code' : [], 'state' : []}\n",
    "visa = { 'code': [], 'type' : []}\n",
    "\n",
    "#united states seems to be missing in the i94 country mapping\n",
    "country['code'].append(888)\n",
    "country['country'].append('United States')\n",
    "with open('I94_SAS_Labels_Descriptions.SAS') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    isCountry = isVisa = isAddr = isPort = isMode = False\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        if line.startswith('value i94cntyl'):\n",
    "            isCountry = True\n",
    "            isVisa = isAddr = isPort = isMode = False\n",
    "        elif line.startswith('value $i94prtl'):\n",
    "            isPort = True\n",
    "            isCountry = isVisa = isAddr = isMode = False\n",
    "        elif line.startswith('value i94model'):\n",
    "            isMode = True\n",
    "            isCountry = isVisa = isAddr = isPort = False\n",
    "        elif line.startswith('value i94addrl'):\n",
    "            isAddr = True\n",
    "            isCountry = isVisa = isPort = isMode = False\n",
    "        elif line.startswith('/* I94VISA'):\n",
    "            isVisa = True\n",
    "            isCountry = isAddr = isPort = isMode = False\n",
    "            \n",
    "        try:\n",
    "            if isCountry and '=' in line:\n",
    "                data = line.split('=')\n",
    "                if len(data) >= 2:\n",
    "                    code = int(data[0].strip())\n",
    "                    c = data[1].replace(';', '').replace(\"'\", '').strip()\n",
    "                    if c:\n",
    "                        country['country'].append(c.title())\n",
    "                        country['code'].append(code)\n",
    "            elif isPort and '=' in line:\n",
    "                data = line.split('=')\n",
    "                if len(data) >= 2:\n",
    "                    code = data[0].replace(\"'\", \"\").strip()\n",
    "                    pair = data[1].strip().replace(\"'\", \"\").split(',')\n",
    "                    if len(pair) >= 2:\n",
    "                        city = pair[0].strip()\n",
    "                        state = pair[1].strip()\n",
    "                    elif len(pair) == 1:\n",
    "                        city = pair[0].strip()\n",
    "                        state = 'NA'\n",
    "                    if code and city and state:\n",
    "                        port['code'].append(code)\n",
    "                        port['city'].append(city.title()) #convert each word to lower case, except first character\n",
    "                        port['state'].append(state.title())\n",
    "            elif isMode and '=' in line:\n",
    "                data = line.split('=')\n",
    "                if len(data) >= 2:\n",
    "                    code = int(data[0].strip())\n",
    "                    m = data[1].replace(';', '').replace(\"'\", \"\").strip()\n",
    "                    if m:\n",
    "                        mode['code'].append(code)\n",
    "                        mode['mode'].append(m)\n",
    "            elif isAddr and '=' in line:\n",
    "                data = line.split('=')\n",
    "                if len(data) >= 2:\n",
    "                    code = data[0].replace(\"'\", '').strip()\n",
    "                    state = data[1].replace(\"'\", '').replace(\";\", '').strip()\n",
    "                    if code and state:\n",
    "                        address['code'].append(code)\n",
    "                        address['state'].append(state.title())\n",
    "            elif isVisa and '=' in line:\n",
    "                data = line.split('=')\n",
    "                if len(data) >= 2:\n",
    "                    code = int(data[0].strip())\n",
    "                    t = data[1].replace(';', '').replace(\"'\", \"\").strip()\n",
    "                    if t:\n",
    "                        visa['code'].append(code)\n",
    "                        visa['type'].append(t)\n",
    "        except Exception as e:\n",
    "            print (f'fail, catch exception: {e}, processing line: {line}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: long (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n",
      "+----+--------------------+\n",
      "|code|             country|\n",
      "+----+--------------------+\n",
      "| 888|       United States|\n",
      "| 582|Mexico Air Sea, A...|\n",
      "| 236|         Afghanistan|\n",
      "| 101|             Albania|\n",
      "| 316|             Algeria|\n",
      "+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create Country dataframe\n",
    "countryDf = spark.createDataFrame(pd.DataFrame(country))\n",
    "countryDf.printSchema()\n",
    "countryDf.count()\n",
    "countryDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n",
      "+----+--------------------+-----+\n",
      "|code|                city|state|\n",
      "+----+--------------------+-----+\n",
      "| ALC|               Alcan|   Ak|\n",
      "| ANC|           Anchorage|   Ak|\n",
      "| BAR|Baker Aaf - Baker...|   Ak|\n",
      "| DAC|       Daltons Cache|   Ak|\n",
      "| PIZ|Dew Station Pt La...|   Ak|\n",
      "+----+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create Port dataframe\n",
    "portDf = spark.createDataFrame(pd.DataFrame(port))\n",
    "portDf.printSchema()\n",
    "portDf.count()\n",
    "portDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: long (nullable = true)\n",
      " |-- mode: string (nullable = true)\n",
      "\n",
      "+----+------------+\n",
      "|code|        mode|\n",
      "+----+------------+\n",
      "|   1|         Air|\n",
      "|   2|         Sea|\n",
      "|   3|        Land|\n",
      "|   9|Not reported|\n",
      "+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create transportation mode dataframe\n",
    "modeDf = spark.createDataFrame(pd.DataFrame(mode))\n",
    "modeDf.printSchema()\n",
    "modeDf.count()\n",
    "modeDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n",
      "+----+----------+\n",
      "|code|     state|\n",
      "+----+----------+\n",
      "|  AL|   Alabama|\n",
      "|  AK|    Alaska|\n",
      "|  AZ|   Arizona|\n",
      "|  AR|  Arkansas|\n",
      "|  CA|California|\n",
      "+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create address dataframe\n",
    "addressDf = spark.createDataFrame(pd.DataFrame(address))\n",
    "addressDf.printSchema()\n",
    "addressDf.count()\n",
    "addressDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n",
      "+----+--------+\n",
      "|code|    type|\n",
      "+----+--------+\n",
      "|   1|Business|\n",
      "|   2|Pleasure|\n",
      "|   3| Student|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create visa dataframe\n",
    "visaDf = spark.createDataFrame(pd.DataFrame(visa))\n",
    "visaDf.printSchema()\n",
    "visaDf.count()\n",
    "visaDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# clean i94 dataframe\n",
    "#join with countryDf\n",
    "spark_i94_df = i94Df\n",
    "\n",
    "spark_i94_df = spark_i94_df.select(col('cicid').cast(IntegerType()).alias('id'), \n",
    "                                   col('i94yr').cast(IntegerType()).alias('year'), \\\n",
    "                                   col('i94mon').cast(IntegerType()).alias('month'), \\\n",
    "                                   col('i94cit').cast(IntegerType()).alias('citizen'), \\\n",
    "                                   col('i94res').cast(IntegerType()).alias('resident'), \\\n",
    "                                   col('i94port').alias('port'), \\\n",
    "                                   col('arrdate').cast(IntegerType()), \\\n",
    "                                   col('i94mode').cast(IntegerType()).alias('transport_mode'), \\\n",
    "                                   col('i94addr').alias('arrival_state'), \\\n",
    "                                   col('depdate').cast(IntegerType()), \\\n",
    "                                   col('i94visa').cast(IntegerType()).alias('visa_category'), \\\n",
    "                                   col('occup').alias('occupation'), \\\n",
    "                                   col('biryear').cast(IntegerType()).alias('birth_year'), \\\n",
    "                                   col('gender'), \\\n",
    "                                   col('i94bir').cast(IntegerType()).alias('age'), \\\n",
    "                                   col('visatype').alias('visa_type')\n",
    "                                  )\n",
    "#convert transportation mode\n",
    "#spark_i94_df = spark_i94_df.join(modeDf, col('transport_mode') == modeDf.code, how='left').drop('code', 'transport_mode') \\\n",
    "#                           .withColumnRenamed(\"mode\", \"transport_mode\")\n",
    "#convert visa category\n",
    "#spark_i94_df = spark_i94_df.join(visaDf, col('visa_category') == visaDf.code, how = 'left').drop('code', 'visa_category') \\\n",
    "#                           .withColumnRenamed('type', 'visa_type')\n",
    "#translate state code\n",
    "#spark_i94_df = spark_i94_df.join(addressDf, col('arrival_state') == addressDf.code, how='left').drop('code', 'arrival_state') \\\n",
    "#                           .withColumnRenamed('state', 'arrival_state')\n",
    "#translate port city/state\n",
    "spark_i94_df = spark_i94_df.join(portDf, col('port') == portDf.code).drop('port', 'code') \\\n",
    "                           .withColumnRenamed('city', 'port_city').withColumnRenamed('state', 'port_state')\n",
    "#convert arrival date\n",
    "get_date = udf(lambda x: (datetime.datetime(1960, 1, 1).date() + datetime.timedelta(x)).isoformat() if x else None)\n",
    "spark_i94_df = spark_i94_df.withColumn('arrival_date', get_date(spark_i94_df.arrdate)) \\\n",
    "                           .withColumn('departure_date', get_date(spark_i94_df.depdate)) \\\n",
    "                           .drop('arrdate', 'depdate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- citizen: integer (nullable = true)\n",
      " |-- resident: integer (nullable = true)\n",
      " |-- transport_mode: integer (nullable = true)\n",
      " |-- arrival_state: string (nullable = true)\n",
      " |-- visa_category: integer (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- birth_year: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- visa_type: string (nullable = true)\n",
      " |-- port_city: string (nullable = true)\n",
      " |-- port_state: string (nullable = true)\n",
      " |-- arrival_date: string (nullable = true)\n",
      " |-- departure_date: string (nullable = true)\n",
      "\n",
      "+------+----+-----+-------+--------+--------------+-------------+-------------+----------+----------+------+---+---------+---------+----------+------------+--------------+\n",
      "|    id|year|month|citizen|resident|transport_mode|arrival_state|visa_category|occupation|birth_year|gender|age|visa_type|port_city|port_state|arrival_date|departure_date|\n",
      "+------+----+-----+-------+--------+--------------+-------------+-------------+----------+----------+------+---+---------+---------+----------+------------+--------------+\n",
      "| 13351|2016|    4|    116|     116|             1|           ME|            1|      null|      1948|     M| 68|       B1|   Bangor|        Me|  2016-04-01|    2016-04-03|\n",
      "| 26320|2016|    4|    131|     131|             1|           OH|            1|      null|      1970|     M| 46|       B1|   Bangor|        Me|  2016-04-01|    2016-04-08|\n",
      "| 51159|2016|    4|    148|     112|             1|           NE|            1|      null|      1967|     M| 49|       B1|   Bangor|        Me|  2016-04-01|    2016-04-02|\n",
      "| 51160|2016|    4|    148|     112|             1|           NE|            1|      null|      1972|     M| 44|       B1|   Bangor|        Me|  2016-04-01|    2016-04-02|\n",
      "|154968|2016|    4|    582|     582|             1|           ME|            1|      null|      1974|     M| 42|       B1|   Bangor|        Me|  2016-04-01|    2016-04-02|\n",
      "|155457|2016|    4|    582|     582|             1|           ME|            1|      null|      1974|     M| 42|       B1|   Bangor|        Me|  2016-04-01|    2016-04-02|\n",
      "|155458|2016|    4|    582|     582|             1|           ME|            1|      null|      1975|     M| 41|       B1|   Bangor|        Me|  2016-04-01|    2016-04-02|\n",
      "|155459|2016|    4|    582|     582|             1|           ME|            1|      null|      1977|     M| 39|       B1|   Bangor|        Me|  2016-04-01|    2016-04-02|\n",
      "|155460|2016|    4|    582|     582|             1|           ME|            1|      null|      1980|     M| 36|       B1|   Bangor|        Me|  2016-04-01|    2016-04-02|\n",
      "|155461|2016|    4|    582|     582|             1|           ME|            1|      null|      1987|     M| 29|       B1|   Bangor|        Me|  2016-04-01|    2016-04-02|\n",
      "+------+----+-----+-------+--------+--------------+-------------+-------------+----------+----------+------+---+---------+---------+----------+------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_i94_df.printSchema()\n",
    "spark_i94_df.show(10)\n",
    "spark_i94_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# clean weather dataframe\n",
    "spark_weather_df = weatherDf.filter(col('Country') == 'United States') \\\n",
    "                            .select(col('dt'), col(\"AverageTemperature\").alias('average_temperature'), \\\n",
    "                                    col('City').alias('city'), col('Country').alias('country')\n",
    "                                    ).orderBy(asc('dt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: timestamp (nullable = true)\n",
      " |-- average_temperature: double (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n",
      "+-------------------+-------------------+----------+-------------+\n",
      "|                 dt|average_temperature|      city|      country|\n",
      "+-------------------+-------------------+----------+-------------+\n",
      "|1743-11-01 00:00:00|  8.129999999999999|   Atlanta|United States|\n",
      "|1743-11-01 00:00:00|              5.371|Bridgeport|United States|\n",
      "|1743-11-01 00:00:00|              3.209|     Akron|United States|\n",
      "|1743-11-01 00:00:00|              2.208| Ann Arbor|United States|\n",
      "|1743-11-01 00:00:00|              5.339| Arlington|United States|\n",
      "|1743-11-01 00:00:00|              5.339|Alexandria|United States|\n",
      "|1743-11-01 00:00:00|              3.264| Allentown|United States|\n",
      "|1743-11-01 00:00:00|              3.015|    Aurora|United States|\n",
      "|1743-11-01 00:00:00|              5.339| Baltimore|United States|\n",
      "|1743-11-01 00:00:00|             10.572|Birmingham|United States|\n",
      "+-------------------+-------------------+----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "687289"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_weather_df.printSchema()\n",
    "spark_weather_df.show(10)\n",
    "spark_weather_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# clean demographics dataframe\n",
    "spark_demographics_df = demographicsDf.select(col('City').alias('city'), col('State').alias('state'), \\\n",
    "                                              col(\"Median Age\").alias('median_age'), col('Male Population').alias('male_population'), \\\n",
    "                                              col('Female Population').alias('female_population'), \\\n",
    "                                              col('Total Population').alias('total_population'), \\\n",
    "                                              col('Number of Veterans').alias('verterans_population'), \\\n",
    "                                              col('Foreign-born').alias('foreign-born'), \\\n",
    "                                              col('Average Household Size').alias('average_household_size'), \\\n",
    "                                              col('State Code').alias('state_code'), \\\n",
    "                                              col('Race').alias('race'), \\\n",
    "                                              col('Count').alias('count') \\\n",
    "                                             ).orderBy(asc('state_code'), asc('city'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# explode the Race column\n",
    "race_df = spark_demographics_df.select(\"city\", \"state_code\", \"race\", \"count\") \\\n",
    "                               .groupby(\"city\", \"state_code\") \\\n",
    "                               .pivot(\"race\") \\\n",
    "                               .agg(first(\"count\")).orderBy(asc('state_code'), asc('city'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+---------------------------------+-----+-------------------------+------------------+------+\n",
      "|        city|state_code|American Indian and Alaska Native|Asian|Black or African-American|Hispanic or Latino| White|\n",
      "+------------+----------+---------------------------------+-----+-------------------------+------------------+------+\n",
      "|   Anchorage|        AK|                            36339|36825|                    23107|             27261|212696|\n",
      "|  Birmingham|        AL|                             1319| 1500|                   157985|              8940| 51728|\n",
      "|      Dothan|        AL|                              656| 1175|                    23243|              1704| 43516|\n",
      "|      Hoover|        AL|                             null| 4759|                    18191|              3430| 61869|\n",
      "|  Huntsville|        AL|                             1755| 6566|                    61561|             10887|121904|\n",
      "|      Mobile|        AL|                             2816| 5518|                    96397|              5229| 93755|\n",
      "|  Montgomery|        AL|                             1277| 6518|                   121360|              6648| 73545|\n",
      "|  Tuscaloosa|        AL|                              261| 2733|                    42331|              2475| 52603|\n",
      "|Fayetteville|        AR|                             2058| 4707|                     6927|              5535| 68830|\n",
      "|  Fort Smith|        AR|                             2993| 6228|                     9851|             17104| 66004|\n",
      "+------------+----------+---------------------------------+-----+-------------------------+------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "race_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#join the exploded table, and remove duplicates\n",
    "spark_demographics_df = spark_demographics_df.join(race_df, ['city', 'state_code'], how='left')\n",
    "spark_demographics_df = spark_demographics_df.select(col('city'), col('state'), col('median_age'), col('male_population'), \\\n",
    "                             col('female_population'), col('total_population'), col('verterans_population'), \\\n",
    "                             col('foreign-born').alias('foreign_born'), col('average_household_size'), col('state_code'), \\\n",
    "                             col('American Indian and Alaska Native').alias('native_population'), \\\n",
    "                             col('Asian').alias('asian_population'), col('Black or African-American').alias('black_population'), \\\n",
    "                             col('Hispanic or Latino').alias('latino_population'), col('White').alias('white_population')) \\\n",
    "                             .dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- median_age: double (nullable = true)\n",
      " |-- male_population: integer (nullable = true)\n",
      " |-- female_population: integer (nullable = true)\n",
      " |-- total_population: integer (nullable = true)\n",
      " |-- verterans_population: integer (nullable = true)\n",
      " |-- foreign_born: integer (nullable = true)\n",
      " |-- average_household_size: double (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- native_population: integer (nullable = true)\n",
      " |-- asian_population: integer (nullable = true)\n",
      " |-- black_population: integer (nullable = true)\n",
      " |-- latino_population: integer (nullable = true)\n",
      " |-- white_population: integer (nullable = true)\n",
      "\n",
      "+----------+-------+----------+---------------+-----------------+----------------+--------------------+------------+----------------------+----------+-----------------+----------------+----------------+-----------------+----------------+\n",
      "|      city|  state|median_age|male_population|female_population|total_population|verterans_population|foreign_born|average_household_size|state_code|native_population|asian_population|black_population|latino_population|white_population|\n",
      "+----------+-------+----------+---------------+-----------------+----------------+--------------------+------------+----------------------+----------+-----------------+----------------+----------------+-----------------+----------------+\n",
      "| Anchorage| Alaska|      32.2|         152945|           145750|          298695|               27492|       33258|                  2.77|        AK|            36339|           36825|           23107|            27261|          212696|\n",
      "|Birmingham|Alabama|      35.6|         102122|           112789|          214911|               13212|        8258|                  2.21|        AL|             1319|            1500|          157985|             8940|           51728|\n",
      "|    Dothan|Alabama|      38.9|          32172|            35364|           67536|                6334|        1699|                  2.59|        AL|              656|            1175|           23243|             1704|           43516|\n",
      "|    Hoover|Alabama|      38.5|          38040|            46799|           84839|                4819|        8229|                  2.58|        AL|             null|            4759|           18191|             3430|           61869|\n",
      "|Huntsville|Alabama|      38.1|          91764|            97350|          189114|               16637|       12691|                  2.18|        AL|             1755|            6566|           61561|            10887|          121904|\n",
      "+----------+-------+----------+---------------+-----------------+----------------+--------------------+------------+----------------------+----------+-----------------+----------------+----------------+-----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_demographics_df.printSchema()\n",
    "spark_demographics_df.show(5)\n",
    "spark_demographics_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# clean airport dataframe\n",
    "# only us airport\n",
    "# split coordinate column into latitude and longitude columns\n",
    "spark_airport_df = airportDf.filter(col('iso_country') == 'US') \\\n",
    "                            .withColumn(\"latitude\", trim(split(col(\"coordinates\"), \",\").getItem(0))) \\\n",
    "                            .withColumn(\"longitude\", trim(split(col(\"coordinates\"), \",\").getItem(1))) \\\n",
    "                            .withColumn(\"state\", trim(split(col(\"iso_region\"), \"-\").getItem(1))) \\\n",
    "                            .select(col('ident').alias('id'), col('type'), col('name'), col('elevation_ft').alias('elevation'), \\\n",
    "                                    col('iso_country').alias('country'), col('state'), col('municipality'),\n",
    "                                    col('gps_code'), col('iata_code'), col('local_code'), col('latitude').cast(DecimalType(9,6)),\n",
    "                                    col('longitude').cast(DecimalType(9,6)))\n",
    "                                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- latitude: decimal(9,6) (nullable = true)\n",
      " |-- longitude: decimal(9,6) (nullable = true)\n",
      "\n",
      "+----+-------------+--------------------+---------+-------+-----+------------+--------+---------+----------+-----------+---------+\n",
      "|  id|         type|                name|elevation|country|state|municipality|gps_code|iata_code|local_code|   latitude|longitude|\n",
      "+----+-------------+--------------------+---------+-------+-----+------------+--------+---------+----------+-----------+---------+\n",
      "| 00A|     heliport|   Total Rf Heliport|       11|     US|   PA|    Bensalem|     00A|     null|       00A| -74.933601|40.070801|\n",
      "|00AA|small_airport|Aero B Ranch Airport|     3435|     US|   KS|       Leoti|    00AA|     null|      00AA|-101.473911|38.704022|\n",
      "|00AK|small_airport|        Lowell Field|      450|     US|   AK|Anchor Point|    00AK|     null|      00AK|-151.695999|59.949200|\n",
      "|00AL|small_airport|        Epps Airpark|      820|     US|   AL|     Harvest|    00AL|     null|      00AL| -86.770302|34.864799|\n",
      "|00AR|       closed|Newport Hospital ...|      237|     US|   AR|     Newport|    null|     null|      null| -91.254898|35.608700|\n",
      "+----+-------------+--------------------+---------+-------+-----+------------+--------+---------+----------+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22757"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_airport_df.printSchema()\n",
    "spark_airport_df.show(5)\n",
    "spark_airport_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "1. The design is to support analyse of immigration data, such as traveller's citizenship, date of arrival, which city is more popular\n",
    "2. The data is organized in a star schema, with fact and dimension table\n",
    "3. There is a factImmigration table, containing immigration related information\n",
    "4. There are 7 dimension tables, dimWeather, dimVisaCategory, dimAirport, dimCountry, dimCity, dimState, dimTransportMode\n",
    "\n",
    "#### 3.2 Schema\n",
    "the schema of the table is as follows\n",
    "\n",
    "##### 3.2.1, Fact Table: factImmigration\n",
    "\n",
    "| Column |  Data Type | Constraint |\n",
    "| ------ | ---------- | ---------- |\n",
    "| cicid | integer | PK |\n",
    "| year| integer| |\n",
    "| month | integer | |\n",
    "| citizen | integer | FK (dimCountry) |\n",
    "| resident | integer | FK (dimCountry) |\n",
    "| transport_mode | integer | FK (dimTransportMode) |\n",
    "| arrival_state | string | FK (dimState) |\n",
    "| visa_category | integer | FK (dimVisaCategory) |\n",
    "| occupation | string | |\n",
    "| birth_year | integer | |\n",
    "| gender | string | |\n",
    "| age| integer | |\n",
    "| visa_type | string | |\n",
    "| port_city | integer | FK (dimCity) |\n",
    "| arrival_date | date | |\n",
    "| departure_date | date | |\n",
    "\n",
    "##### 3.2.2, Dimension Table: DimWeather\n",
    "\n",
    "| Column | Data Type | Constraint |\n",
    "| ------ | --------- | ---------- |\n",
    "| city | integer | PK, FK(DimCity) |\n",
    "| date | date | PK |\n",
    "| average_temperature | double | |\n",
    "\n",
    "##### 3.2.3, Dimension Table: dimVisaCategory\n",
    "\n",
    "| Column | Data Type | Constraint |\n",
    "| ------ | --------- | ---------- |\n",
    "| code | integer | PK |\n",
    "| type | string | |\n",
    "\n",
    "\n",
    "##### 3.2.4, Dimension Table: DimAirPort\n",
    "\n",
    "| column | Data Type | Constraint |\n",
    "| ------ | --------- | ---------- |\n",
    "| id | string | PK |\n",
    "| type | string | |\n",
    "| name | string | |\n",
    "| elevation | integer | |\n",
    "| country | integer | FK(DimCountry) |\n",
    "| state | string | FK(DimState) |\n",
    "| municipality | string | |\n",
    "| gps_code | string | |\n",
    "| iata_code | string | |\n",
    "| local_code | string | |\n",
    "| latitude | decimal(9,6) | |\n",
    "| longitude | decimal(9,6) |  |\n",
    "\n",
    "##### 3.2.5, Dimension Table: DimCountry\n",
    "\n",
    "| Column | Data Type | Contraint |\n",
    "| ------ | --------- | --------- |\n",
    "| code   | integer   | PK |\n",
    "| name   | string    |           |\n",
    "\n",
    "\n",
    "##### 3.2.6, Dimension Table: DimCity\n",
    "\n",
    "| column | Data Type | Constraint |\n",
    "| ------ | --------- | ---------- |\n",
    "| city | integer | PK |\n",
    "| name | string | |\n",
    "| state | string | FK(DimState) |\n",
    "| median_age | double | |\n",
    "| male_population | integer | |\n",
    "| female_population | integer | |\n",
    "| total_population | integer | |\n",
    "| verterans_population | integer | |\n",
    "| foreign-born | integer | |\n",
    "| average_household_size | double | |\n",
    "| native_population | integer | |\n",
    "| asian_population | integer | |\n",
    "| black_population | integer | |\n",
    "| latino_population | integer | |\n",
    "| white_population | integer | |\n",
    " \n",
    "##### 3.2.7, Dimension Table: DimState\n",
    "\n",
    "| column | Data Type | Constraint |\n",
    "|------- | --------- | ---------- |\n",
    "| code   | string    | PK |\n",
    "| name   | string    |            |\n",
    "\n",
    "##### 3.2.8, Dimension Table: DimTransportMode\n",
    "\n",
    "| column | Data Type | Constraint |\n",
    "| ------ | --------- | ---------- |\n",
    "| code | integer | PK |\n",
    "| mode | string |  |\n",
    " \n",
    "#### 3.3 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model.\n",
    "\n",
    "The Fact and Dimension Table will be created in Amazon RedShift cluster. \n",
    "\n",
    "##### 4.1.1 Create Statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create db connection\n",
    "conn = psycopg2.connect(f\"host={HOST} dbname={DB_NAME} user={DB_USER} password={DB_PASSWORD} port={DB_PORT}\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#drop fact and dimension table\n",
    "dimState_drop = \"DROP TABLE IF EXISTS dimState\"\n",
    "dimTransportMode_drop = \"DROP TABLE IF EXISTS dimTransportMode\"\n",
    "dimCountry_drop = \"DROP TABLE IF EXISTS dimCountry\"\n",
    "dimVisaCategory_drop = \"DROP TABLE IF EXISTS dimVisaCategory\"\n",
    "dimCity_drop = \"DROP TABLE IF EXISTS dimCity\"\n",
    "dimWeather_drop = \"DROP TABLE IF EXISTS dimWeather\"\n",
    "dimAirPort_drop = \"DROP TABLE IF EXISTS dimAirPort\"\n",
    "factImmigration_drop = \"DROP TABLE IF EXISTS factImmigration\"\n",
    "\n",
    "# create fact and dimension table\n",
    "dimState_create = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS dimState (\n",
    "code varchar PRIMARY KEY,\n",
    "name varchar(32)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "dimTransportMode_create = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS dimTransportMode (\n",
    "code int PRIMARY KEY,\n",
    "mode varchar(64)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "dimCountry_create = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS dimCountry (\n",
    "code int PRIMARY KEY,\n",
    "name varchar(64)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "dimVisaCategory_create = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS dimVisaCategory (\n",
    "code int PRIMARY KEY,\n",
    "type varchar(64)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "dimCity_create = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS dimCity (\n",
    "city bigint PRIMARY KEY,\n",
    "name varchar(128),\n",
    "state varchar,\n",
    "median_age float,\n",
    "male_population bigint,\n",
    "female_population bigint,\n",
    "total_population bigint,\n",
    "verterans_population bigint,\n",
    "foreign_born bigint,\n",
    "average_household_size float,\n",
    "native_population bigint,\n",
    "asian_population bigint,\n",
    "black_population bigint,\n",
    "latino_population bigint,\n",
    "white_population bigint,\n",
    "FOREIGN KEY(state) REFERENCES dimState(code)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "dimWeather_create = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS dimWeather (\n",
    "city bigint NOT NULL REFERENCES dimCity(city),\n",
    "\"date\" date NOT NULL,\n",
    "average_temperature float,\n",
    "PRIMARY KEY(city, date)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "dimAirPort_create = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS dimAirport (\n",
    "id varchar PRIMARY KEY,\n",
    "type varchar(128),\n",
    "name varchar(128) NOT NULL,\n",
    "elevation int,\n",
    "country int REFERENCES dimCountry(code),\n",
    "state varchar REFERENCES dimState(code),\n",
    "municipality varchar(128),\n",
    "gps_code varchar(64),\n",
    "iata_code varchar(64),\n",
    "local_code varchar(64),\n",
    "latitude numeric(9,6),\n",
    "longitude numeric(9,6)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "factImmigration_create = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS factImmigration (\n",
    "cicid bigint PRIMARY KEY,\n",
    "year int,\n",
    "month int,\n",
    "citizen int REFERENCES dimCountry(code),\n",
    "resident int REFERENCES dimCountry(code),\n",
    "transport_mode int REFERENCES dimTransportMode(code),\n",
    "arrival_state varchar REFERENCES dimState(code),\n",
    "visa_category int REFERENCES dimVisaCategory(code),\n",
    "occupation varchar(128),\n",
    "birth_year int,\n",
    "gender char(1),\n",
    "age int,\n",
    "visa_type varchar(32),\n",
    "port_city bigint REFERENCES dimCity(city),\n",
    "arrival_date date,\n",
    "departure_date date\n",
    ");\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#drop existing tables in redshift db\n",
    "table_drop_list = [ factImmigration_drop, dimAirPort_drop, dimWeather_drop, dimCity_drop, dimVisaCategory_drop, dimTransportMode_drop, \\\n",
    "                    dimCountry_drop, dimState_drop]\n",
    "\n",
    "for query in table_drop_list:\n",
    "    cur.execute(query)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create tables in redshift db\n",
    "table_create_list = [ dimState_create, dimCountry_create, dimTransportMode_create, dimVisaCategory_create, dimCity_create, \\\n",
    "                      dimWeather_create, dimAirPort_create, factImmigration_create]\n",
    "\n",
    "for query in table_create_list:\n",
    "    cur.execute(query)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# setup parameter for spark dataframe to write to redshift\n",
    "mode = \"append\"\n",
    "url = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "conn = create_engine(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.1.2, populate fact and dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#populate dimState table\n",
    "addressDf.select(col('code'), col('state').alias('name')).toPandas().to_sql('dimstate', con = conn, index=False, if_exists=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#populate dimCountry table\n",
    "countryDf.select(col('code'), col('country').alias('name')).toPandas().to_sql('dimcountry', con=conn, index=False, if_exists=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#populate dimTransportMode\n",
    "modeDf.toPandas().to_sql('dimtransportmode', con=conn, index=False, if_exists=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#populate dimVisaCategory\n",
    "visaDf.toPandas().to_sql('dimvisacategory', con = conn, index = False, if_exists = mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#populate dimCity table\n",
    "city_df = spark_demographics_df.drop('state') \\\n",
    "                          .withColumnRenamed('state_code', 'state') \\\n",
    "                          .withColumnRenamed('city', 'name') \\\n",
    "                          .withColumn('city', monotonically_increasing_id())\n",
    "city_df.toPandas().to_sql('dimcity', con = conn, index=False, if_exists=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#to limit row count, otherwise query taking long time\n",
    "city_df = city_df.filter(col('name') == 'New York')\n",
    "\n",
    "#populate dimWeather table\n",
    "#calculates the average daily temperature for us cities\n",
    "spark_daily_weather_df = spark_weather_df.filter(col('Country') == 'United States').withColumnRenamed('city', 'cityname') \\\n",
    "                                          .select(col('cityname'), to_date(col('dt')).alias('date'), col('average_temperature')) \\\n",
    "                                          .dropna() \\\n",
    "                                          .groupBy('cityname', 'date').agg(mean('average_temperature').alias('average_temperature'))\n",
    "weather_df = spark_daily_weather_df.join(city_df, spark_daily_weather_df.cityname == city_df.name) \\\n",
    "                             .drop_duplicates().select(col('city'),col('date'), col('average_temperature'))\n",
    "\n",
    "\n",
    "weather_df.toPandas().to_sql('dimweather', con = conn, index = False, if_exists = mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#popualte dimAirport table\n",
    "uscode = countryDf.filter(col('country') == 'United States').collect()\n",
    "airport_df = spark_airport_df.filter(col('country') == 'US').drop('country').withColumn('country', lit(uscode[0][0]))\n",
    "\n",
    "#to limit row count, otherwise query taking too long\n",
    "#airport_df = airport_df.filter(col('state') == 'NY')\n",
    "\n",
    "airport_df.toPandas().to_sql('dimairport', con=conn, index=False, if_exists=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#populate factImmigration table\n",
    "i94_df = spark_i94_df\n",
    "#to limit rows, otherwise it is taking a long time\n",
    "#i94_df = i94_df.filter(col('visa_type') == 'F1')\n",
    "\n",
    "i94_df = i94_df.join(city_df.select(col('city'),col('name')) , i94_df.port_city == city_df.name) \\\n",
    "               .drop('name', 'port_city', 'port_state') \\\n",
    "               .select(col('id').alias('cicid'), col('year'), col('month'), col('citizen'), col('resident'), \\\n",
    "                       col('transport_mode'), col('arrival_state'), col('visa_category'), \\\n",
    "                       col('occupation'), col('birth_year'), col('gender'), col('age'), \\\n",
    "                       col('visa_type'), to_date(col('arrival_date')).alias('arrival_date'), \\\n",
    "                       to_date(col('departure_date')).alias('departure_date'), \\\n",
    "                       col('city').alias('port_city'))\n",
    "\n",
    "i94_df.toPandas().to_sql('factimmigration', con = conn, index = False, if_exists=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "conn.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create db connection\n",
    "conn = psycopg2.connect(f\"host={HOST} dbname={DB_NAME} user={DB_USER} password={DB_PASSWORD} port={DB_PORT}\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table dimState has 55 rows\n",
      "Table dimCountry has 290 rows\n",
      "Table dimVisaCategory has 3 rows\n",
      "Table dimTransportMode has 4 rows\n",
      "Table dimWeather has 3119 rows\n",
      "Table dimAirport has 668 rows\n",
      "Table dimCity has 596 rows\n",
      "Table factImmigration has 5659 rows\n"
     ]
    }
   ],
   "source": [
    "# Perform quality checks here\n",
    "# notice the original data set is too big, when storing data to the database, only a subset was stored otherwise it will take\n",
    "# too long to run\n",
    "\n",
    "\n",
    "#row count check\n",
    "for name in ['dimState', 'dimCountry', 'dimVisaCategory', 'dimTransportMode', 'dimWeather', 'dimAirport', 'dimCity', 'factImmigration']:\n",
    "    query = f\"select count(*) from {name}\"\n",
    "    cur.execute(query)\n",
    "    result = cur.fetchone()\n",
    "    print (f\"Table {name} has {result[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state code: AL, name: Alabama\n",
      "state code: AR, name: Arkansas\n",
      "state code: AZ, name: Arizona\n",
      "state code: CA, name: California\n",
      "state code: CO, name: Colorado\n",
      "state code: CT, name: Connecticut\n",
      "state code: DC, name: Dist. Of Columbia\n",
      "state code: DE, name: Delaware\n",
      "state code: FL, name: Florida\n",
      "state code: GA, name: Georgia\n",
      "state code: HI, name: Hawaii\n",
      "state code: IA, name: Iowa\n",
      "state code: ID, name: Idaho\n",
      "state code: IL, name: Illinois\n",
      "state code: IN, name: Indiana\n",
      "state code: KS, name: Kansas\n",
      "state code: KY, name: Kentucky\n",
      "state code: LA, name: Louisiana\n",
      "state code: MA, name: Massachusetts\n",
      "state code: MD, name: Maryland\n",
      "state code: ME, name: Maine\n",
      "state code: MI, name: Michigan\n",
      "state code: MN, name: Minnesota\n",
      "state code: MO, name: Missouri\n",
      "state code: MS, name: Mississippi\n",
      "state code: MT, name: Montana\n",
      "state code: NC, name: N. Carolina\n",
      "state code: ND, name: N. Dakota\n",
      "state code: NE, name: Nebraska\n",
      "state code: NH, name: New Hampshire\n",
      "state code: NJ, name: New Jersey\n",
      "state code: NM, name: New Mexico\n",
      "state code: NV, name: Nevada\n",
      "state code: NY, name: New York\n",
      "state code: OH, name: Ohio\n",
      "state code: OK, name: Oklahoma\n",
      "state code: OR, name: Oregon\n",
      "state code: PA, name: Pennsylvania\n",
      "state code: RI, name: Rhode Island\n",
      "state code: SC, name: S. Carolina\n",
      "state code: TN, name: Tennessee\n",
      "state code: TX, name: Texas\n",
      "state code: UT, name: Utah\n",
      "state code: VA, name: Virginia\n",
      "state code: VT, name: Vermont\n",
      "state code: WA, name: Washington\n",
      "state code: WI, name: Wisconson\n"
     ]
    }
   ],
   "source": [
    "# verify we can retrieve the name of ths state from fact table and state table\n",
    "query = \"select distinct s.code, s.name as state from factImmigration f join dimState s on s.code = f.arrival_state order by f.arrival_state\"\n",
    "cur.execute(query)\n",
    "result = cur.fetchall()\n",
    "for row in result:\n",
    "        print (f\"state code: {row[0]}, name: {row[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country, code: 101, name: Albania\n",
      "country, code: 316, name: Algeria\n",
      "country, code: 102, name: Andorra\n",
      "country, code: 324, name: Angola\n",
      "country, code: 518, name: Antigua-Barbuda\n",
      "country, code: 687, name: Argentina\n",
      "country, code: 438, name: Australia\n",
      "country, code: 103, name: Austria\n",
      "country, code: 152, name: Azerbaijan\n",
      "country, code: 512, name: Bahamas\n",
      "country, code: 298, name: Bahrain\n",
      "country, code: 274, name: Bangladesh\n",
      "country, code: 513, name: Barbados\n",
      "country, code: 153, name: Belarus\n",
      "country, code: 104, name: Belgium\n",
      "country, code: 386, name: Benin\n",
      "country, code: 509, name: Bermuda\n",
      "country, code: 242, name: Bhutan\n",
      "country, code: 688, name: Bolivia\n",
      "country, code: 164, name: Bosnia-Herzegovina\n",
      "country, code: 336, name: Botswana\n",
      "country, code: 689, name: Brazil\n",
      "country, code: 105, name: Bulgaria\n",
      "country, code: 310, name: Cameroon\n",
      "country, code: 690, name: Chile\n",
      "country, code: 245, name: China, Prc\n",
      "country, code: 311, name: Collapsed Tanzania (Should Not Show)\n",
      "country, code: 691, name: Colombia\n",
      "country, code: 385, name: Congo\n",
      "country, code: 575, name: Costa Rica\n",
      "country, code: 165, name: Croatia\n",
      "country, code: 218, name: Cyprus\n",
      "country, code: 140, name: Czech Republic\n",
      "country, code: 108, name: Denmark\n",
      "country, code: 519, name: Dominica\n",
      "country, code: 585, name: Dominican Republic\n",
      "country, code: 692, name: Ecuador\n",
      "country, code: 368, name: Egypt\n",
      "country, code: 576, name: El Salvador\n",
      "country, code: 399, name: Equatorial Guinea\n",
      "country, code: 372, name: Eritrea\n",
      "country, code: 109, name: Estonia\n",
      "country, code: 369, name: Ethiopia\n",
      "country, code: 110, name: Finland\n",
      "country, code: 111, name: France\n",
      "country, code: 387, name: Gabon\n",
      "country, code: 338, name: Gambia\n",
      "country, code: 154, name: Georgia\n",
      "country, code: 339, name: Ghana\n",
      "country, code: 113, name: Greece\n",
      "country, code: 520, name: Grenada\n",
      "country, code: 577, name: Guatemala\n",
      "country, code: 382, name: Guinea\n",
      "country, code: 603, name: Guyana\n",
      "country, code: 586, name: Haiti\n",
      "country, code: 528, name: Honduras\n",
      "country, code: 206, name: Hong Kong\n",
      "country, code: 114, name: Hungary\n",
      "country, code: 115, name: Iceland\n",
      "country, code: 213, name: India\n",
      "country, code: 204, name: Indonesia\n",
      "country, code: 574, name: Invalid: Canada\n",
      "country, code: 249, name: Iran\n",
      "country, code: 250, name: Iraq\n",
      "country, code: 116, name: Ireland\n",
      "country, code: 251, name: Israel\n",
      "country, code: 117, name: Italy\n",
      "country, code: 388, name: Ivory Coast\n",
      "country, code: 514, name: Jamaica\n",
      "country, code: 209, name: Japan\n",
      "country, code: 253, name: Jordan\n",
      "country, code: 201, name: Kampuchea\n",
      "country, code: 155, name: Kazakhstan\n",
      "country, code: 340, name: Kenya\n",
      "country, code: 732, name: Kosovo\n",
      "country, code: 272, name: Kuwait\n",
      "country, code: 156, name: Kyrgyzstan\n",
      "country, code: 203, name: Laos\n",
      "country, code: 118, name: Latvia\n",
      "country, code: 255, name: Lebanon\n",
      "country, code: 370, name: Liberia\n",
      "country, code: 381, name: Libya\n",
      "country, code: 121, name: Luxembourg\n",
      "country, code: 273, name: Malaysia\n",
      "country, code: 392, name: Mali\n",
      "country, code: 582, name: Mexico Air Sea, And Not Reported (I-94, No Land Arrivals)\n",
      "country, code: 122, name: Monaco\n",
      "country, code: 299, name: Mongolia\n",
      "country, code: 735, name: Montenegro\n",
      "country, code: 332, name: Morocco\n",
      "country, code: 329, name: Mozambique\n",
      "country, code: 257, name: Nepal\n",
      "country, code: 123, name: Netherlands\n",
      "country, code: 464, name: New Zealand\n",
      "country, code: 579, name: Nicaragua\n",
      "country, code: 390, name: Niger\n",
      "country, code: 343, name: Nigeria\n",
      "country, code: 275, name: North Korea\n",
      "country, code: 124, name: Norway\n",
      "country, code: 256, name: Oman\n",
      "country, code: 258, name: Pakistan\n",
      "country, code: 743, name: Palestine\n",
      "country, code: 504, name: Panama\n",
      "country, code: 693, name: Paraguay\n",
      "country, code: 694, name: Peru\n",
      "country, code: 260, name: Philippines\n",
      "country, code: 107, name: Poland\n",
      "country, code: 126, name: Portugal\n",
      "country, code: 297, name: Qatar\n",
      "country, code: 127, name: Romania\n",
      "country, code: 376, name: Rwanda\n",
      "country, code: 261, name: Saudi Arabia\n",
      "country, code: 391, name: Senegal\n",
      "country, code: 745, name: Serbia\n",
      "country, code: 348, name: Sierra Leone\n",
      "country, code: 207, name: Singapore\n",
      "country, code: 141, name: Slovakia\n",
      "country, code: 166, name: Slovenia\n",
      "country, code: 373, name: South Africa\n",
      "country, code: 129, name: Spain\n",
      "country, code: 244, name: Sri Lanka\n",
      "country, code: 522, name: St. Kitts-Nevis\n",
      "country, code: 523, name: St. Lucia\n",
      "country, code: 524, name: St. Vincent-Grenadines\n",
      "country, code: 130, name: Sweden\n",
      "country, code: 131, name: Switzerland\n",
      "country, code: 268, name: Taiwan\n",
      "country, code: 159, name: Tajikistan\n",
      "country, code: 263, name: Thailand\n",
      "country, code: 516, name: Trinidad And Tobago\n",
      "country, code: 323, name: Tunisia\n",
      "country, code: 352, name: Uganda\n",
      "country, code: 162, name: Ukraine\n",
      "country, code: 296, name: United Arab Emirates\n",
      "country, code: 135, name: United Kingdom\n",
      "country, code: 695, name: Uruguay\n",
      "country, code: 163, name: Uzbekistan\n",
      "country, code: 696, name: Venezuela\n",
      "country, code: 266, name: Vietnam\n",
      "country, code: 301, name: Zaire\n",
      "country, code: 315, name: Zimbabwe\n"
     ]
    }
   ],
   "source": [
    "# verify we can retrieve the name of the country from fact table and country table\n",
    "query = \"select distinct c.code, c.name as country from factImmigration f join dimCountry c on f.citizen = c.code order by c.name\"\n",
    "cur.execute(query)\n",
    "result = cur.fetchall()\n",
    "for row in result:\n",
    "    print (f\"country, code: {row[0]}, name: {row[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### additional queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: NY, count: 3441\n",
      "state: CA, count: 295\n",
      "state: NJ, count: 251\n",
      "state: MA, count: 240\n",
      "state: PA, count: 212\n"
     ]
    }
   ],
   "source": [
    "# top 5 popular state that has the highest immigration count\n",
    "query = \"select count(*) as count, arrival_state from factImmigration group by arrival_state order by count desc limit 5\"\n",
    "cur.execute(query)\n",
    "result = cur.fetchall()\n",
    "for row in result:\n",
    "    print (f\"state: {row[1]}, count: {row[0]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "The data dictionary for the Fact and Dimension tables are as below\n",
    "\n",
    "##### Fact Table: factImmigration\n",
    "\n",
    "| Column |  Description | Source |\n",
    "| ------ | ------------ | ------ |\n",
    "| cicid | unique identifier in I94 immigration data set | I94 immigration sas7dat files |\n",
    "| year| year of arrival| I94 immigration sas7dat files |\n",
    "| month | month of arrival | I94 immigration sas7dat files |\n",
    "| citizen | country of citizenship, id which is used to join with dimCountry Table | dimCountry table |\n",
    "| resident | country of residence, id which is used to join with dimCountry table | dimCountry table |\n",
    "| transport_mode | transport mode id which is used to join with dimTransportMode table | dimTransportMode table |\n",
    "| arrival_state | arrival state code used to join with dimState table | dimState table |\n",
    "| visa_category | visa category, whcih is used to join with dimVisaCategory table | dimVisaCategory table |\n",
    "| occupation | occupation | I94 immigration sas7dat files |\n",
    "| birth_year | year of birth | I94 immigration sas7dat files |\n",
    "| gender | gender, M or F | I94 immigration sas7dat files |\n",
    "| age| age | I94 immigration sas7dat files |\n",
    "| visa_type | visa type | I94 immigration sas7dat files |\n",
    "| port_city | port of arrival, city id which is used to join with dimCity table | dimCity table |\n",
    "| arrival_date | date of arrival | I94 immigration sas7dat files |\n",
    "| departure_date | date of departure | I94 immigration sas7dat files |\n",
    "\n",
    "##### Dimension Table: DimWeather\n",
    "\n",
    "| Column | Description | Source |\n",
    "| ------ | ----------- | ------ |\n",
    "| city | city id used to join with dimCity | dimCity table |\n",
    "| date | date of the temperature recording | GlobalLandTemperatureByCities.csv |\n",
    "| average_temperature | average temperature for a given date | GlobalLandTemperatureByCities.csv |\n",
    "\n",
    "##### Dimension Table: dimVisaCategory\n",
    "\n",
    "| Column | Description | Source |\n",
    "| ------ | ----------- | ------ |\n",
    "| code | visa category code | I94 immigration sas7dat files |\n",
    "| type | visa category type | I94 immigration sas7dat files |\n",
    "\n",
    "\n",
    "##### Dimension Table: DimAirPort\n",
    "\n",
    "| column | Description | Source |\n",
    "| ------ | ----------- | ------ |\n",
    "| id | airport unique id | airport-codes_csv.csv |\n",
    "| type | string | airport-codes_csv.csv |\n",
    "| name | string | airport-codes_csv.csv |\n",
    "| elevation | integer | airport-codes_csv.csv |\n",
    "| country | country code which is used to join with dimCountry table | dimCountry table |\n",
    "| state | state code of the airport, which is used to join with dimState table | dimState table |\n",
    "| municipality | municpal of the airport | airport-codes_csv.csv |\n",
    "| gps_code | gps identifier of the airport | airport-codes_csv.csv |\n",
    "| iata_code |  IATA code of the airport | airport-codes_csv.csv |\n",
    "| local_code | local identifier of the airport | airport-codes_csv.csv |\n",
    "| latitude | latitude of the airport | airport-codes_csv.csv |\n",
    "| longitude | longitude of the airport |  airport-codes_csv.csv |\n",
    "\n",
    "##### Dimension Table: DimCountry\n",
    "\n",
    "| Column | Description | Source |\n",
    "| ------ | ----------- | ------ |\n",
    "| code   | country code | I94_SAS_Labels_Descriptions.SAS |\n",
    "| name   | country name | I94_SAS_Labels_Descriptions.SAS |\n",
    "\n",
    "\n",
    "##### Dimension Table: DimCity\n",
    "\n",
    "| column | Description | Source |\n",
    "| ------ | ----------- | ------ |\n",
    "| city | city id, which is generated automatically and serve as primary key | auto generated |\n",
    "| name | name of the city | us-cities-demographics.csv |\n",
    "| state | state code, which is used to join with dimState table | dimState table |\n",
    "| median_age | median age of the given city | us-cities-demographics.csv |\n",
    "| male_population | male population of the given city | us-cities-demographics.csv |\n",
    "| female_population | female population of the given city | us-cities-demographics.csv |\n",
    "| total_population | total population of the given city | us-cities-demographics.csv |\n",
    "| verterans_population | population of verterans in the given city | us-cities-demographics.csv |\n",
    "| foreign-born | population of foreign born in the given city | us-cities-demographics.csv |\n",
    "| average_household_size | average household size in the given city | us-cities-demographics.csv |\n",
    "| native_population | native population in the given city | us-cities-demographics.csv |\n",
    "| asian_population | asian population in the given city | us-cities-demographics.csv |\n",
    "| black_population | black population in the given city | us-cities-demographics.csv |\n",
    "| latino_population | latino population in the given city | us-cities-demographics.csv |\n",
    "| white_population | white population in the given city | us-cities-demographics.csv |\n",
    " \n",
    "##### Dimension Table: DimState\n",
    "\n",
    "| column | Description | Source |\n",
    "|------- | ----------- | ------ |\n",
    "| code   | state code    | I94 immigration sas7dat files |\n",
    "| name   | state name    | I94 immigration sas7dat files |\n",
    "\n",
    "##### Dimension Table: DimTransportMode\n",
    "\n",
    "| column | Description | Source |\n",
    "| ------ | ----------- | ------ |\n",
    "| code | transportation mode id | I94 immigration sas7dat files |\n",
    "| mode | transporttation mode  | I94 immigration sas7dat files |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "\n",
    "  The aim of the project is to support queries of I94 immigration data. Apache Spark was used to extract data from various data source, clean up the data and load the data into redshift database.\n",
    "  \n",
    "  Spark was used, because it can be run in a cluster, and as data volume increase we can add more machines to handle the volume.\n",
    "  The data was stored in redshift becasue of its easy of use, setup and performance. Also it support sql queries that most users is already familar with.\n",
    "  \n",
    "  \n",
    "* Propose how often the data should be updated and why.\n",
    "\n",
    "  Since the I94 data is monthly, it will be appropriate to incrementally udpate the data monthly.\n",
    "  \n",
    "  \n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " \n",
    "   If the data was to be increased by 100x fold, we can add more nodes to the spark cluster(EMR) to handle the etl portion. On top of that we should also increase the cluster size of the redshift database backend, employ more powerful machines with more memory and disk space, so it can handle the anticipated increase in data volume.\n",
    "   \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " \n",
    "   If the data needs to be updated daily by 7am every day, then we should use automated workflow solution like airflow. which will send out notification (via slack channel, email or other means) to notify if the job has been successful completed, or need attention by certain time so manual intervention can kick in to ensure the updated will meet the 7am deadline each day.\n",
    "   \n",
    "   \n",
    " * The database needed to be accessed by 100+ people.\n",
    " \n",
    "   If the number of users accessing the data has to be accessed by 100+ people, we can upgrade the tier of the redshift database server. Using more powerful machines, more cpu's and memory to handle the increase in user population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
